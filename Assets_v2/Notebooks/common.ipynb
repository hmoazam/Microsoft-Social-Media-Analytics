{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": "common",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run \"config\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import tweepy\n",
        "import urllib.parse\n",
        "import sys, time, json, requests, uuid\n",
        "from tweepy import API\n",
        "from tweepy import Cursor\n",
        "from tweepy import OAuthHandler\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "from azure.cosmos import CosmosClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def authenticate_client():\n",
        "    \"\"\"\n",
        "    Returns: text analytics client\n",
        "    \"\"\"\n",
        "    ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=TEXT_ANALYTICS_ENDPOINT, \n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "text_analytics_client = authenticate_client()\n",
        "# TODO: Add opinion mining\n",
        "def get_sentiment(inp_text):\n",
        "    #Parameters: \n",
        "    #  inp_text: text to analyze\n",
        "    #Returns:\n",
        "    #  sentiment, sentiment score\n",
        "    documents = [inp_text]\n",
        "    response = text_analytics_client.analyze_sentiment(documents = documents)[0]  \n",
        "    try:\n",
        "        overallscore = response.confidence_scores.positive + (0.5*response.confidence_scores.neutral) # check logic of this\n",
        "        return response.sentiment, overallscore\n",
        "    except Exception as err:\n",
        "        print(\"Encountered Sentiment exception. {}\".format(err))\n",
        "        return \"Neutral\",0.5\n",
        "def get_ner(inp_text):\n",
        "    #Parameters: \n",
        "    #  inp_text: text to analyze\n",
        "    #Returns:\n",
        "    #  NER Results as a list of dictionaries with keys: text, category, subcategory, length, offset, confidence\n",
        "    try:\n",
        "        documents = [inp_text]\n",
        "        result = text_analytics_client.recognize_entities(documents = documents)[0]  \n",
        "        return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result.entities]\n",
        "    except Exception as err:\n",
        "        print(\"Encountered NER exception. {}\".format(err))\n",
        "    return []\n",
        "def get_key_phrases(inp_text):\n",
        "    #Parameters: \n",
        "    #  inp_text: text to analyze\n",
        "    #Returns:\n",
        "    #  List of key phrases\n",
        "    try:\n",
        "      documents = [inp_text]\n",
        "      response = text_analytics_client.extract_key_phrases(documents = documents)[0] \n",
        "      if not response.is_error:\n",
        "          return response.key_phrases\n",
        "      else:\n",
        "          print(response.id, response.error)\n",
        "    except Exception as err:\n",
        "      print(\"Encountered Translation exception. {}\".format(err))\n",
        "    return []\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def update_cosmos(objects, container): # insert tweets/users to cosmos\n",
        "    for obj in objects:\n",
        "        if obj:\n",
        "            response = container.upsert_item(body=obj) # use upsert so that insert or update\n",
        "    print(\"Inserted data to cosmos\")\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Tweet Entities processing\n",
        "import requests\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import json\n",
        "def get_maps_response(inp):\n",
        "    url = \"https://atlas.microsoft.com/search/fuzzy/json?&subscription-key=\"+MAPS_KEY+\"&api-version=1.0&language=en-US&query=\"+inp\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def get_translation(inp_text, to_languages):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    inp_text: text to be translated\n",
        "    to_languages: list of languages to translate to\n",
        "    Returns: {lang_code: translation}, language code of the original text\n",
        "    Call to translator cognitive service detects language and translates to the target languages. \n",
        "    Result is a dictionary of language codes to translated text, along with the language detected.\n",
        "    \"\"\"\n",
        "    # Translator setup\n",
        "    translator_path = \"/translate\"\n",
        "    translator_url = TRANSLATOR_ENDPOINT + translator_path\n",
        "    params = {\n",
        "    \"api-version\": \"3.0\",\n",
        "    \"to\": to_languages\n",
        "    }\n",
        "    headers = {\n",
        "    'Ocp-Apim-Subscription-Key': TRANSLATOR_KEY,\n",
        "    'Ocp-Apim-Subscription-Region': TRANSLATOR_REGION,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "    # create and send request\n",
        "    body = [{\n",
        "    'text': inp_text\n",
        "    }]\n",
        "    request = requests.post(translator_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "    # only ever one string sent for translation, so only list of length 1\n",
        "    try:\n",
        "        from_language = response[0][\"detectedLanguage\"][\"language\"]\n",
        "        translations = response[0][\"translations\"]\n",
        "        res = {} # dict with language as key and translated text as value e.g. {\"ar\": \"arabic text\"}\n",
        "        for trans in translations:\n",
        "            res[trans['to']] = trans['text']\n",
        "        #res[from_language] = inp_text # also include the original text and language in translations - overkill?\n",
        "        return res, from_language # return the translated text, as well as the language it was translated from\n",
        "    except Exception as err:\n",
        "        print(\"Encountered an exception. {}\".format(err))\n",
        "        return err\n",
        ""
      ],
      "attachments": {}
    }
  ]
}
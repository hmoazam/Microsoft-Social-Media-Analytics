{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "smasparkpool",
              "session_id": 165,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-03-05T09:10:42.4005502Z",
              "session_start_time": "2022-03-05T09:10:42.4456879Z",
              "execution_start_time": "2022-03-05T09:13:55.1553472Z",
              "execution_finish_time": "2022-03-05T09:13:57.0178552Z"
            },
            "text/plain": "StatementMeta(smasparkpool, 165, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import os\n",
        "from azure.cosmos import CosmosClient, PartitionKey\n",
        "from pyspark.sql.types import StructType, StructField, LongType, StringType, DateType, TimestampType,FloatType\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "import family\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "from datetime import date, timedelta \n",
        "from datetime import datetime as _datetime \n",
        "\n",
        "import numpy as np\n",
        "import datetime, time\n",
        "from dateutil.parser import parse\n",
        "import re,string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": 184,
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-03-05T13:03:21.7153367Z",
              "session_start_time": null,
              "execution_start_time": "2022-03-05T13:03:24.3534522Z",
              "execution_finish_time": "2022-03-05T13:03:24.3537928Z"
            },
            "text/plain": "StatementMeta(, 184, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DB_NAME: String = synsqlpoolsmapoc\n"
        }
      ],
      "metadata": {},
      "source": [
        "%run \"config\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Connect to Cosmos\n",
        "client = CosmosClient(COSMOS_URL, {'masterKey': COSMOS_KEY})\n",
        "database = client.get_database_client(COSMOS_DATABASE_NAME)\n",
        "\n",
        "tweet_container_client = database.get_container_client(container=COSMOS_CONTAINER_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "smasparkpool",
              "session_id": 184,
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-03-05T13:04:03.5372762Z",
              "session_start_time": null,
              "execution_start_time": "2022-03-05T13:04:03.9181148Z",
              "execution_finish_time": "2022-03-05T13:04:04.329063Z"
            },
            "text/plain": "StatementMeta(smasparkpool, 184, 22, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "last_inserted_ts = 0\r\n",
        "\r\n",
        "jdbc_url = \"jdbc:sqlserver://\" + SYNAPSE_WORKSPACE_NAME + \".sql.azuresynapse.net:1433;database=\" + DB_NAME + \";encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\r\n",
        "\r\n",
        "jdbcDF = spark.read \\\r\n",
        "    .format(\"jdbc\") \\\r\n",
        "    .option(\"url\", jdbc_url) \\\r\n",
        "    .option(\"query\", \"SELECT MAX(inserted_to_CosmosDB_ts) AS outp FROM dbo.Tweets\") \\\r\n",
        "    .option(\"user\", SQL_USERNAME) \\\r\n",
        "    .option(\"password\", SQL_PASSWORD) \\\r\n",
        "    .load()\r\n",
        "\r\n",
        "try:\r\n",
        "    last_inserted_ts = jdbcDF.first()[0]\r\n",
        "except: \r\n",
        "    last_inserted_ts = 0\r\n",
        "\r\n",
        "if not(last_inserted_ts): # if the table is empty get back None\r\n",
        "    last_inserted_ts = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "smasparkpool",
              "session_id": 184,
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-03-05T13:04:09.8154922Z",
              "session_start_time": null,
              "execution_start_time": "2022-03-05T13:04:10.1868939Z",
              "execution_finish_time": "2022-03-05T13:04:10.5937824Z"
            },
            "text/plain": "StatementMeta(smasparkpool, 184, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "1646475627"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "last_inserted_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val dfLastInsertedInDW = spark.read.synapsesql(DB_NAME+\".dbo.[Tweets]\")\n",
        "dfLastInsertedInDW.createOrReplaceTempView(\"dfLastInsertedInDW\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "last_inserted_ts=0\r\n",
        "dfLastInsertedInDW = spark.sql(\"select * from dfLastInsertedInDW\")\r\n",
        "try:\r\n",
        "   last_inserted_ts=dfLastInsertedInDW.agg(max(\"inserted_to_CosmosDB_ts\"))\r\n",
        "except:\r\n",
        "    last_inserted_ts=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "now = datetime.datetime.now()\r\n",
        "then = now - datetime.timedelta(days=2)\r\n",
        "#span_ms = int(time.mktime(then.timetuple())) # only for testing. Disregard\r\n",
        "dt_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "LANGUAGE_CODES={\"All\":\"\",\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Assamese\":\"as\",\"Bangla\":\"bn\",\"Bosnian(Latin)\":\"bs\",\"Bulgarian\":\"bg\",\"Cantonese(Traditional)\":\"yue\",\"Catalan\":\"ca\",\"Chinese Simplified\":\"zh-Hans\",\"Chinese Traditional\":\"zh-Hant\",\"Croatian\":\"hr\",\"Czech\":\"cs\",\"Dari\":\"prs\",\"Danish\":\"da\",\"Dutch\":\"nl\",\"English\":\"en\",\"Estonian\":\"et\",\"Fijian\":\"fj\",\"Filipino\":\"fil\",\"Finnish\":\"fi\",\"French\":\"fr\",\"German\":\"de\",\"Greek\":\"el\",\"Gujarati\":\"gu\",\"Haitian Creole\":\"ht\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Hmong Daw\":\"mww\",\"Hungarian\":\"hu\",\"Icelandic\":\"is\",\"Indonesian\":\"id\",\"Irish\":\"ga\",\"Italian\":\"it\",\"Japanese\":\"ja\",\"Kannada\":\"kn\",\"Kazakh\":\"kk\",\"Klingon\":\"tlh-Latn\",\"Klingon(plqaD)\":\"tlh-Piqd\",\"Korean\":\"ko\",\"Kurdish(Central)\":\"ku\",\"Kurdish(Northern)\":\"kmr\",\"Latvian\":\"lv\",\"Lithuanian\":\"lt\",\"Malagasy\":\"mg\",\"Malay\":\"ms\",\"Malayalam\":\"ml\",\"Maltese\":\"mt\",\"Maori\":\"mi\",\"Marathi\":\"mr\",\"Norwegian\":\"nb\",\"Odia\":\"or\",\"Pashto\":\"ps\",\"Persian\":\"fa\",\"Polish\":\"pl\",\"Portuguese(Brazil)\":\"pt-br\",\"Portuguese(Portugal)\":\"pt-pt\",\"Punjabi\":\"pa\",\"Queretaro Otomi\":\"otq\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Samoan\":\"sm\",\"Serbian(Cyrillic)\":\"sr-Cyrl\",\"Serbian(Latin)\":\"sr-Latn\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Spanish\":\"es\",\"Swahili\":\"sw\",\"Swedish\":\"sv\",\"Tahitian\":\"ty\",\"Tamil\":\"ta\",\"Telugu\":\"te\",\"Thai\":\"th\",\"Tongan\":\"to\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Urdu\":\"ur\",\"Vietnamese\":\"vi\",\"Welsh\":\"cy\",\"Yucatec Maya\":\"yua\"}\r\n",
        "LANGUAGE_CODES = {v: k for (k, v) in LANGUAGE_CODES.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# query = \"SELECT items.id,items.subtopic, items.full_text, items.user, items.lang, items.place, items.retweet_count, items.favorite_count, items.entities, items.possibly_sensitive, items.translations, items.classification, items.possible_news, items.key_phrases, items.named_entities, items.topics, items.created_at,  items.topic_names, items.query_list, items.topic_relevancy_list, items.worthiness, items._ts, items.in_reply_to_status_id, items.in_reply_to_user_id, items.source from items where items.document_type = 'tweet' and items._ts >= \"  + str(last_inserted_ts) #+ \" and items.timestamp_ms >= '\"+str(span_ms) + \"'\"\n",
        "# TODO: This cell needs cleaning - lots of repeated accesses etc. Even datetime.now\n",
        "query = \"SELECT items.id,items.subtopic, items.full_text, items.user, items.userid, items.lang, items.place, items.retweet_count, items.favorite_count, items.entities, items.topics, items.translations, items.key_phrases, items.named_entities, items.sentiment, items.topickey, items.created_at, items.query, items.inserted_to_CosmosDB_ts, items.in_reply_to_status_id, items.in_reply_to_user_id, items.source from items where items.document_type = 'tweet' and items.inserted_to_CosmosDB_ts >= \"  + str(last_inserted_ts) # Need to see which fields we want from cosmos. items.inserted_to_CosmosDB_ts used to be _ts - check the same?\n",
        "lst, lstsearches, lsthashtags, lsthandles, lstmedia, lstKeyPhrases, lstEntities, lstSentiment,  lstTranslations, lstURLs = ([] for i in range(10))\n",
        "lstEntityAnalysis=[]\n",
        "for posts in tweet_container_client.query_items(query,enable_cross_partition_query=True ): # loop over list of json documents from cosmos, and fill in the lists - translations/keyphrases etc. Using lists to build data model\n",
        "  # using conditionals as not every object has all the fields below.\n",
        "  city = ''\n",
        "  country = ''\n",
        "  Language = ''\n",
        "  id_str = posts[\"id\"]\n",
        "\n",
        "  for l in posts['translations']:\n",
        "    lstTranslations.append([id_str, l, posts['translations'][l], _datetime.now()])\n",
        "  #for l in posts['key_phrases']:\n",
        "  #  for phrase in posts['key_phrases'][l]:\n",
        "  #    lstKeyPhrases.append([id_str, phrase, LANGUAGE_CODES.get(l, \"unknown\"), _datetime.now()])\n",
        "  for l in posts['named_entities']:\n",
        "    for entity in posts['named_entities'][l]:\n",
        "      if 'country_azuremaps' in entity.keys() and 'country_code_azuremaps' in entity.keys():\n",
        "        lstEntities.append([id_str, entity['category'], entity['subcategory'], entity['text'],  LANGUAGE_CODES.get(l, \"unknown\"), float(entity['confidence_score']),entity[\"country_azuremaps\"],entity[\"country_code_azuremaps\"], _datetime.now() ])\n",
        "      else:\n",
        "        lstEntities.append([id_str, entity['category'], entity['subcategory'], entity['text'],  LANGUAGE_CODES.get(l, \"unknown\"), float(entity['confidence_score']),None,None, _datetime.now() ])\n",
        "  Language = LANGUAGE_CODES.get(posts['lang'], 'Unknown')\n",
        "\n",
        "  if posts['place'] is None:\n",
        "    city = 'NA'\n",
        "    country = 'NA'  \n",
        "  else:\n",
        "      city = posts[\"place\"]['name']\n",
        "      country = posts[\"place\"]['country']\n",
        "  #if posts['lang'] == 'en':\n",
        "  #  Language = 'English'\n",
        "  #elif posts['lang'] == 'ar':\n",
        "  #  Language = 'Arabic'\n",
        "  #else:\n",
        "  #    Language = 'Unknown'\n",
        "  for hashtag in posts['entities']['hashtags']:\n",
        "    lsthashtags.append([id_str, hashtag['text'], _datetime.now()])\n",
        "  for user_mention in posts['entities']['user_mentions']:\n",
        "    lsthandles.append([id_str, user_mention['screen_name'], _datetime.now()])\n",
        "  for urls in posts['entities']['urls']:\n",
        "    lstURLs.append([id_str, urls['url'], urls['expanded_url'], urls['display_url'],  _datetime.now()])\n",
        "  if 'media' in posts['entities']:\n",
        "    for media in posts['entities']['media']:\n",
        "      lstmedia.append([id_str, media['media_url'], _datetime.now()])\n",
        "#   if 'classification' in posts: # for al jazeera\n",
        "#     for classification in posts['classification']:\n",
        "#       lstclassifications.append([id_str, classification, _datetime.now()])\n",
        "  if \"sentiment\" in posts.keys():\n",
        "    lstSentiment.append([id_str, posts[\"sentiment\"][\"sentiment\"], posts[\"sentiment\"][\"score\"], _datetime.now()])\n",
        "  adjustedCreatedDateTime = parse(posts[\"created_at\"])+ timedelta(hours=3) \n",
        "  if \"originalid\" in posts.keys():\n",
        "    idforlink=posts[\"originalid\"]\n",
        "  else:\n",
        "    idforlink=id_str\n",
        "  #append tweet\n",
        "  lst.append([id_str, \n",
        "              posts[\"full_text\"],\n",
        "              #posts[\"user\"][\"id_str\"], \n",
        "              posts[\"userid\"],\n",
        "\t      posts[\"topickey\"],\n",
        "\t      posts[\"subtopic\"],\n",
        "              city, country,               \n",
        "              posts[\"retweet_count\"], \n",
        "              posts[\"favorite_count\"], Language,  \n",
        "              0,#int(posts[\"worthiness\"]), \n",
        "              posts[\"source\"], posts[\"source\"], \n",
        "              '', #factcheckURL, - for al jazeera, don't need\n",
        "              'https://twitter.com/' + posts['user']['screen_name'] + '/status/' +idforlink,\n",
        "              True if 'RT' in posts[\"full_text\"] else False, #retweets\n",
        "              \"\",#posts[\"possible_news\"],\n",
        "              posts[\"in_reply_to_status_id\"] if posts[\"in_reply_to_status_id\"] is not None else -1 ,\n",
        "              posts[\"in_reply_to_user_id\"] if posts[\"in_reply_to_user_id\"] is not None else -1,\n",
        "              adjustedCreatedDateTime,\n",
        "              adjustedCreatedDateTime,\n",
        "              _datetime.fromtimestamp(posts[\"inserted_to_CosmosDB_ts\"]),\n",
        "              posts[\"inserted_to_CosmosDB_ts\"],\n",
        "              _datetime.now(),\n",
        "             ])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create Spark Dataframes\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"text\",StringType(),True),  StructField(\"userid\",StringType(),True),  StructField(\"topic\",StringType(),True),  StructField(\"subtopic\",StringType(),True),  StructField(\"city\",StringType(),True),  StructField(\"country\",StringType(),True),  StructField(\"retweets\",LongType(),False),  StructField(\"likes\",LongType(),True),  StructField(\"lang\",StringType(),True),  StructField(\"worthinessScore\",LongType(),True),  StructField(\"fullSource\",StringType(),True),  StructField(\"Source\",StringType(),True),  StructField(\"factCheckURL\",StringType(),True),  StructField(\"tweetURL\",StringType(),True),  StructField(\"isRetweet\",StringType(),True),  StructField(\"possibleNews\",StringType(),True),  StructField(\"replyToStatus\",LongType(),True),  StructField(\"replyToUser\",LongType(),True),  StructField(\"created_date\",DateType(),True),  StructField(\"created_datetime\",TimestampType(),True),  StructField(\"inserted_to_CosmosDB_datetime\",TimestampType(),True),  StructField(\"inserted_to_CosmosDB_ts\",LongType(),True),  StructField(\"inserted_datetime\", TimestampType(), True)])\r\n",
        "dftweets = sqlContext.createDataFrame(lst,schema)\r\n",
        "dftweets.createOrReplaceTempView(\"dftweets\")\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"hashtags\",StringType(),False),  StructField(\"created_datetime\", TimestampType(), False)])\r\n",
        "dfhashtags = spark.createDataFrame(lsthashtags, schema)\r\n",
        "dfhashtags.createOrReplaceTempView(\"dfhashtags\")\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"handles\",StringType(),False),  StructField(\"created_datetime\", TimestampType(), False)])\r\n",
        "dfhandles = spark.createDataFrame(lsthandles,schema)\r\n",
        "dfhandles.createOrReplaceTempView(\"dfhandles\")\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"media\",StringType(),False),  StructField(\"created_datetime\", TimestampType(), False)])\r\n",
        "dfmedia = spark.createDataFrame(lstmedia,schema)\r\n",
        "dfmedia.createOrReplaceTempView(\"dfmedia\")\r\n",
        "# schema = StructType([StructField(\"id\",LongType(),False), \\\r\n",
        "#   StructField(\"classifications\",StringType(),False), \\\r\n",
        "#   StructField(\"created_datetime\", TimestampType(), False)])\r\n",
        "# dfclassifications = spark.createDataFrame(lstclassifications,schema)\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"URL\",StringType(),True),  StructField(\"Expanded_URL\",StringType(),True),  StructField(\"display_URL\",StringType(),True),  StructField(\"created_datetime\", TimestampType(), True)])\r\n",
        "dfURLs = spark.createDataFrame(lstURLs,schema)\r\n",
        "dfURLs.createOrReplaceTempView(\"dfURLs\")\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"Language\",StringType(),False),  StructField(\"Text\",StringType(),False),  StructField(\"created_datetime\", TimestampType(), True)])\r\n",
        "dfTranslations = spark.createDataFrame(lstTranslations,schema)\r\n",
        "dfTranslations.createOrReplaceTempView(\"dfTranslations\")\r\n",
        "#schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"KeyPhrase\",StringType(),False),  StructField(\"Language\",StringType(),True),  StructField(\"created_datetime\", TimestampType(), True)])\r\n",
        "#dfKeyPhrases = spark.createDataFrame(lstKeyPhrases,schema)\r\n",
        "#dfKeyPhrases.createOrReplaceTempView(\"dfKeyPhrases\")\r\n",
        "schema = StructType([StructField(\"id\",StringType(),False),  StructField(\"category\",StringType(),False),  StructField(\"subcategory\",StringType(),True),  StructField(\"value\",StringType(),True),  StructField(\"Language\",StringType(),True),  StructField(\"confidence_score\",FloatType(),True),  StructField(\"country_azuremaps\",StringType(),True),  StructField(\"country_code_azuremaps\",StringType(),True),  StructField(\"created_datetime\", TimestampType(), True)])\r\n",
        "dfEntities = spark.createDataFrame(lstEntities,schema)\r\n",
        "dfEntities.createOrReplaceTempView(\"dfEntities\")\r\n",
        "schema = StructType([StructField(\"id\", StringType(), False),                     StructField(\"sentiment\", StringType(), True),                     StructField(\"overallscore\", FloatType(), True),                     StructField(\"created_datetime\", TimestampType(), True)                    ])\r\n",
        "dfSentiment = spark.createDataFrame(lstSentiment, schema)\r\n",
        "dfSentiment.createOrReplaceTempView(\"dfSentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "if dftweets.count() == 0:\n",
        "  print(\"Didn't capture new tweets.\")\n",
        "  # dbutils.notebook.exit(0)\n",
        "  #mssparkutils.notebook.exit(\"no more tweets\")\n",
        "else:\n",
        "  print(str(dftweets.count() ) + \" tweets to process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synapse Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val scala_dftweets = spark.sqlContext.sql (\"select * from dftweets\")\n",
        "scala_dftweets.write.synapsesql(DB_NAME+\".stg.[Tweets]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val scala_dfhashtags = spark.sqlContext.sql (\"select * from dfhashtags\")\n",
        "scala_dfhashtags.write.synapsesql(DB_NAME+\".stg.[Hashtags]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val scala_dfhandles = spark.sqlContext.sql (\"select * from dfhandles\")\n",
        "scala_dfhandles.write.synapsesql(DB_NAME+\".stg.[Handles]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val scala_dfmedia = spark.sqlContext.sql (\"select * from dfmedia\")\n",
        "scala_dfmedia.write.synapsesql(DB_NAME+\".stg.[TweetMedia]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val scala_dfSentiment = spark.sqlContext.sql (\"select * from dfSentiment\")\n",
        "scala_dfSentiment.write.synapsesql(DB_NAME+\".stg.[Sentiments]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val scala_dfURLs = spark.sqlContext.sql (\"select * from dfURLs\")\r\n",
        "scala_dfURLs.write.synapsesql(DB_NAME+\".stg.[TweetURLs]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "val scala_dfTranslations = spark.sqlContext.sql (\"select * from dfTranslations\")\r\n",
        "scala_dfTranslations.write.synapsesql(DB_NAME+\".stg.[Translations]\", Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\n",
        "val scala_dfEntities = spark.sqlContext.sql (\"select * from dfEntities\")\n",
        "scala_dfEntities.write.synapsesql(DB_NAME+\".stg.[TweetsEntities]\", Constants.INTERNAL)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}
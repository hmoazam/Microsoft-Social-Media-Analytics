{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Ingest-RSS-Feeds",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Pipeline params \n",
        "# feed_source = \"https://full.gulf-times.com/Rss/Index\"\n",
        "feed_source = \"https://www.psychologytoday.com/us/blog/singletons/feed\"\n",
        "target_languages = \"English,Arabic\"\n",
        "query_optional = \"\" # split with OR\n",
        "query_required = \"\" # split with AND\n",
        "topic = \"Parenting\"\n",
        "subtopic = \"\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azure.cosmos import CosmosClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "import copy\n",
        "import feedparser\n",
        "import sys, time, json, requests, uuid\n",
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "LANGUAGE_CODES={\"All\":\"\",\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Assamese\":\"as\",\"Bangla\":\"bn\",\"Bosnian(Latin)\":\"bs\",\"Bulgarian\":\"bg\",\"Cantonese(Traditional)\":\"yue\",\"Catalan\":\"ca\",\"Chinese Simplified\":\"zh-Hans\",\"Chinese Traditional\":\"zh-Hant\",\"Croatian\":\"hr\",\"Czech\":\"cs\",\"Dari\":\"prs\",\"Danish\":\"da\",\"Dutch\":\"nl\",\"English\":\"en\",\"Estonian\":\"et\",\"Fijian\":\"fj\",\"Filipino\":\"fil\",\"Finnish\":\"fi\",\"French\":\"fr\",\"German\":\"de\",\"Greek\":\"el\",\"Gujarati\":\"gu\",\"Haitian Creole\":\"ht\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Hmong Daw\":\"mww\",\"Hungarian\":\"hu\",\"Icelandic\":\"is\",\"Indonesian\":\"id\",\"Irish\":\"ga\",\"Italian\":\"it\",\"Japanese\":\"ja\",\"Kannada\":\"kn\",\"Kazakh\":\"kk\",\"Klingon\":\"tlh-Latn\",\"Klingon(plqaD)\":\"tlh-Piqd\",\"Korean\":\"ko\",\"Kurdish(Central)\":\"ku\",\"Kurdish(Northern)\":\"kmr\",\"Latvian\":\"lv\",\"Lithuanian\":\"lt\",\"Malagasy\":\"mg\",\"Malay\":\"ms\",\"Malayalam\":\"ml\",\"Maltese\":\"mt\",\"Maori\":\"mi\",\"Marathi\":\"mr\",\"Norwegian\":\"nb\",\"Odia\":\"or\",\"Pashto\":\"ps\",\"Persian\":\"fa\",\"Polish\":\"pl\",\"Portuguese(Brazil)\":\"pt-br\",\"Portuguese(Portugal)\":\"pt-pt\",\"Punjabi\":\"pa\",\"Queretaro Otomi\":\"otq\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Samoan\":\"sm\",\"Serbian(Cyrillic)\":\"sr-Cyrl\",\"Serbian(Latin)\":\"sr-Latn\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Spanish\":\"es\",\"Swahili\":\"sw\",\"Swedish\":\"sv\",\"Tahitian\":\"ty\",\"Tamil\":\"ta\",\"Telugu\":\"te\",\"Thai\":\"th\",\"Tongan\":\"to\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Urdu\":\"ur\",\"Vietnamese\":\"vi\",\"Welsh\":\"cy\",\"Yucatec Maya\":\"yua\"}\n",
        "target_languages = [LANGUAGE_CODES.get(lang, \"\") for lang in target_languages.split(\",\")]\n",
        "if \"en\" not in target_languages:\n",
        "  target_languages.append(\"en\")\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run \"config\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run \"common\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "client = CosmosClient(COSMOS_URL, {'masterKey': COSMOS_KEY})\n",
        "database = client.get_database_client(COSMOS_DATABASE_NAME)\n",
        "rss_container_client = database.get_container_client(container=COSMOS_RSS_FEEDS_CONTAINER_NAME)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def get_translation(inp_text, to_languages):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    inp_text: text to be translated\n",
        "    to_languages: list of languages to translate to\n",
        "    Returns: {lang_code: translation}, language code of the original text\n",
        "    Call to translator cognitive service detects language and translates to the target languages. \n",
        "    Result is a dictionary of language codes to translated text, along with the language detected.\n",
        "    \"\"\"\n",
        "    # Translator setup\n",
        "    translator_path = \"/translate\"\n",
        "    translator_url = TRANSLATOR_ENDPOINT + translator_path\n",
        "    params = {\n",
        "    \"api-version\": \"3.0\",\n",
        "    \"to\": to_languages\n",
        "    }\n",
        "    headers = {\n",
        "    'Ocp-Apim-Subscription-Key': TRANSLATOR_KEY,\n",
        "    'Ocp-Apim-Subscription-Region': TRANSLATOR_REGION,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "    # create and send request\n",
        "    body = [{\n",
        "    'text': inp_text\n",
        "    }]\n",
        "    request = requests.post(translator_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "    # only ever one string sent for translation, so only list of length 1\n",
        "    try:\n",
        "        from_language = response[0][\"detectedLanguage\"][\"language\"]\n",
        "        translations = response[0][\"translations\"]\n",
        "        res = {} # dict with language as key and translated text as value e.g. {\"ar\": \"arabic text\"}\n",
        "        for trans in translations:\n",
        "            res[trans['to']] = trans['text']\n",
        "        res[from_language] = inp_text # also include the original text and language in translations - overkill?\n",
        "        return res, from_language # return the translated text, as well as the language it was translated from\n",
        "    except Exception as err:\n",
        "        print(\"Encountered an exception. {}\".format(err))\n",
        "        return err\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def authenticate_client():\n",
        "    \"\"\"\n",
        "    Returns: text analytics client\n",
        "    \"\"\"\n",
        "    ta_credential = AzureKeyCredential(TEXT_ANALYTICS_KEY)\n",
        "    text_analytics_client = TextAnalyticsClient(\n",
        "            endpoint=TEXT_ANALYTICS_ENDPOINT, \n",
        "            credential=ta_credential)\n",
        "    return text_analytics_client\n",
        "text_analytics_client = authenticate_client()\n",
        "def get_sentiment(inp_text):\n",
        "    #Parameters: \n",
        "    #  inp_text: text to analyze\n",
        "    #Returns:\n",
        "    #  sentiment, sentiment score\n",
        "    documents = [inp_text]\n",
        "    response = text_analytics_client.analyze_sentiment(documents = documents)[0]  \n",
        "    overallscore = response.confidence_scores.positive + (0.5*response.confidence_scores.neutral) # check logic of this\n",
        "    return response.sentiment, overallscore\n",
        "def get_ner(inp_text):\n",
        "    #Parameters: \n",
        "    #  inp_text: text to analyze\n",
        "    #Returns:\n",
        "    #  NER Results as a list of dictionaries with keys: text, category, subcategory, length, offset, confidence\n",
        "    try:\n",
        "        documents = [inp_text]\n",
        "        result = text_analytics_client.recognize_entities(documents = documents)[0]  \n",
        "        return [{\"text\": x.text, \"category\": x.category, \"subcategory\": x.subcategory, \"length\": x.length, \"offset\": x.offset, \"confidence_score\": x.confidence_score} for x in result.entities]\n",
        "    except Exception as err:\n",
        "        print(\"Encountered exception. {}\".format(err))\n",
        "        return []\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def is_new_entry(id_str, container):\n",
        "    # check if this has already been added to cosmos - if yes don't re-process\n",
        "    search_query = 'select * from items where items.id=\"{0}\"'.format(id_str)\n",
        "    items = list(container.query_items(search_query,enable_cross_partition_query=True))\n",
        "    if len(items) > 0:\n",
        "        return False\n",
        "    return True\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def process_title(title, target_languages):\n",
        "    # translate\n",
        "    translated, detected_lang = get_translation(title, target_languages)  \n",
        "    return translated\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def process_summary(summary, target_languages):\n",
        "    # translate\n",
        "    translated, detected_lang = get_translation(summary, target_languages)\n",
        "    # ner\n",
        "    if detected_lang != \"en\":\n",
        "        named_entities = get_ner(translated[\"en\"])\n",
        "        named_entity_obj = {\"en\": named_entities}\n",
        "        org_language_entities = []\n",
        "        for ent in named_entities:\n",
        "            org_language_ent = copy.deepcopy(ent)\n",
        "            org_language_ent[\"text\"] = get_translation(ent[\"text\"], detected_lang)[0][detected_lang] # entity in the original language\n",
        "            org_language_entities.append(org_language_ent)\n",
        "        named_entity_obj[detected_lang] = org_language_entities\n",
        "    else:\n",
        "        named_entities = get_ner(summary) # list of objects where each object corresponds to an entity\n",
        "        named_entity_obj = {\"en\": named_entities}\n",
        "    for language in target_languages:\n",
        "        if language != \"en\":\n",
        "            tmp_entities = []\n",
        "            for ent in named_entities:\n",
        "                tmp_ = copy.deepcopy(ent)\n",
        "                tmp_[\"text\"] = get_translation(ent[\"text\"],language)[0][language]\n",
        "                tmp_entities.append(tmp_)\n",
        "            named_entity_obj[language]=tmp_entities\n",
        "    # sentiment\n",
        "    sentiment, score = get_sentiment(summary)\n",
        "    return translated, named_entity_obj, sentiment, score\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def contains_keywords(text, query_optional, query_required):\n",
        "    text = text.lower()\n",
        "    required_keywords = query_required.split(\"AND\")\n",
        "    required_flag = True\n",
        "    for keyword in required_keywords:\n",
        "        if not(keyword.lower() in text):\n",
        "            return False # no required words present\n",
        "    # check optional words only if required ones present (or empty)\n",
        "    optional_flag = False\n",
        "    optional_keywords = query_optional.split(\"OR\")\n",
        "    for keyword in optional_keywords:\n",
        "        if keyword.lower() in text: \n",
        "            return True # required words there (or empty) and some optional words\n",
        "    # required words there but no optional words\n",
        "    return (required_flag or optional_flag)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def process_feed(news_feed, container, target_languages, query_optional, query_required, topic, subtopic):\n",
        "    rss_fields = ['link', 'title', 'summary', 'published_parsed', 'img']\n",
        "    res = []\n",
        "    feed_name = news_feed.feed.title\n",
        "    for entry in news_feed.entries:\n",
        "        id_str = str(hash(news_feed.feed.title + entry[\"title\"])) # unique identifier for cosmos db\n",
        "        if is_new_entry(id_str, container):\n",
        "            # process this news feed only if not processed before\n",
        "            article_json = {}\n",
        "            article_json['news_feed'] = feed_name\n",
        "            article_json['id'] = id_str\n",
        "            current_timestamp = datetime.now() # time inserted to cosmos db, for change data capture \n",
        "            timestamp_int = int(time.mktime(current_timestamp.timetuple()))\n",
        "            timestamp_string = current_timestamp.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "            article_json['inserted_to_CosmosDB_at'] = timestamp_string\n",
        "            article_json['inserted_to_CosmosDB_ts'] = timestamp_int\n",
        "            article_json['month_year'] = str(current_timestamp.month) + \"_\" + str(current_timestamp.year)\n",
        "            matched = False\n",
        "            for field in rss_fields:\n",
        "                value = entry.get(field, \"\")\n",
        "                if field == 'title':\n",
        "                    processed_title = process_title(value, target_languages) # title translated to target languages\n",
        "                    article_json['title_translated'] = processed_title\n",
        "                    article_json['title'] = value\n",
        "                elif field == 'summary':\n",
        "                    if value: \n",
        "                        if \"img alt\" in value: # elwatan summary seems to contain img tag in summary field \n",
        "                            value = value[value.find(\">\")+1:].lstrip(\"\\n\").lstrip() # extract summary\n",
        "                            print(value)\n",
        "                        processed_summary = process_summary(value, target_languages)                     \n",
        "                        translations, ner, sentiment, score = processed_summary\n",
        "                        article_json['summary'] = value\n",
        "                        article_json['summary_translations'] = translations\n",
        "                        article_json['summary_ner'] = ner\n",
        "                        article_json['sentiment'] = sentiment\n",
        "                        article_json['sentiment_score'] = score\n",
        "                        # check if had query match\n",
        "                        if contains_keywords(translations[\"en\"], query_optional, query_required): # always need the english translation. Searching for keywords in english only for now\n",
        "                            matched = True\n",
        "                    else: # no summary found\n",
        "                        article_json['summary'] = \"\"\n",
        "                        article_json['summary_translations'] = {k:\"\" for k in target_languages}\n",
        "                        article_json['summary_ner'] = {k:[] for k in target_languages}\n",
        "                        article_json['sentiment'] = \"\"\n",
        "                        article_json['sentiment_score'] = 0 \n",
        "                        matched = True\n",
        "                    # else: \n",
        "                    #     article_json[\"topic\"] = \"\"\n",
        "                    #     article_json[\"subtopic\"] = \"\"\n",
        "                elif field == \"published_parsed\":\n",
        "                    published_ts = int(time.mktime(value))\n",
        "                    article_json['published_ts'] = published_ts\n",
        "                    article_json['published_ts_str'] = (datetime.fromtimestamp(published_ts)).strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "                elif field == \"img\":\n",
        "                    if value != \"\":\n",
        "                        article_json[\"img\"] = value[\"src\"] # the dict also contains the alt text - ideally should store that too, in all the target languages\n",
        "                        print(value[\"src\"])\n",
        "                    else:\n",
        "                        article_json[\"img\"] = \"\"\n",
        "                else:\n",
        "                    article_json[field] = value\n",
        "            if matched:\n",
        "                # assign topic/subtopic\n",
        "                article_json[\"topic\"] = topic\n",
        "                article_json[\"subtopic\"] = subtopic    \n",
        "                res.append(article_json)\n",
        "    return res\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def update_cosmos(objects, container): # insert to cosmos\n",
        "    for obj in objects:\n",
        "        try:\n",
        "            response = container.upsert_item(body=obj) # use upsert so that insert or update\n",
        "            print(\"Inserted to cosmos\")\n",
        "        except: \n",
        "            print(\"Unable to insert\")\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "news_feed = feedparser.parse(feed_source)\n",
        "res = process_feed(news_feed, rss_container_client, target_languages, query_optional, query_required, topic, subtopic)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "update_cosmos(res, rss_container_client)\n",
        ""
      ],
      "attachments": {}
    }
  ]
}
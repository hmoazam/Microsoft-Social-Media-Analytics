{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Ingest-Tweets",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "query = \"\"\n",
        "# https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators - query reference\n",
        "users = \"\"\n",
        "topic = \"\"\n",
        "subtopic = \"\"\n",
        "query_language = \"All\"\n",
        "target_languages = \"English,Arabic\"\n",
        "num_tweets = 100\n",
        "days = 7\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "LANGUAGE_CODES={\"All\":\"\",\"Afrikaans\":\"af\",\"Arabic\":\"ar\",\"Assamese\":\"as\",\"Bangla\":\"bn\",\"Bosnian(Latin)\":\"bs\",\"Bulgarian\":\"bg\",\"Cantonese(Traditional)\":\"yue\",\"Catalan\":\"ca\",\"Chinese Simplified\":\"zh-Hans\",\"Chinese Traditional\":\"zh-Hant\",\"Croatian\":\"hr\",\"Czech\":\"cs\",\"Dari\":\"prs\",\"Danish\":\"da\",\"Dutch\":\"nl\",\"English\":\"en\",\"Estonian\":\"et\",\"Fijian\":\"fj\",\"Filipino\":\"fil\",\"Finnish\":\"fi\",\"French\":\"fr\",\"German\":\"de\",\"Greek\":\"el\",\"Gujarati\":\"gu\",\"Haitian Creole\":\"ht\",\"Hebrew\":\"he\",\"Hindi\":\"hi\",\"Hmong Daw\":\"mww\",\"Hungarian\":\"hu\",\"Icelandic\":\"is\",\"Indonesian\":\"id\",\"Irish\":\"ga\",\"Italian\":\"it\",\"Japanese\":\"ja\",\"Kannada\":\"kn\",\"Kazakh\":\"kk\",\"Klingon\":\"tlh-Latn\",\"Klingon(plqaD)\":\"tlh-Piqd\",\"Korean\":\"ko\",\"Kurdish(Central)\":\"ku\",\"Kurdish(Northern)\":\"kmr\",\"Latvian\":\"lv\",\"Lithuanian\":\"lt\",\"Malagasy\":\"mg\",\"Malay\":\"ms\",\"Malayalam\":\"ml\",\"Maltese\":\"mt\",\"Maori\":\"mi\",\"Marathi\":\"mr\",\"Norwegian\":\"nb\",\"Odia\":\"or\",\"Pashto\":\"ps\",\"Persian\":\"fa\",\"Polish\":\"pl\",\"Portuguese(Brazil)\":\"pt-br\",\"Portuguese(Portugal)\":\"pt-pt\",\"Punjabi\":\"pa\",\"Queretaro Otomi\":\"otq\",\"Romanian\":\"ro\",\"Russian\":\"ru\",\"Samoan\":\"sm\",\"Serbian(Cyrillic)\":\"sr-Cyrl\",\"Serbian(Latin)\":\"sr-Latn\",\"Slovak\":\"sk\",\"Slovenian\":\"sl\",\"Spanish\":\"es\",\"Swahili\":\"sw\",\"Swedish\":\"sv\",\"Tahitian\":\"ty\",\"Tamil\":\"ta\",\"Telugu\":\"te\",\"Thai\":\"th\",\"Tongan\":\"to\",\"Turkish\":\"tr\",\"Ukrainian\":\"uk\",\"Urdu\":\"ur\",\"Vietnamese\":\"vi\",\"Welsh\":\"cy\",\"Yucatec Maya\":\"yua\"}\n",
        "topic = topic.lower()\n",
        "# username = user\n",
        "userslist = users.split(',')\n",
        "query_language = LANGUAGE_CODES.get(query_language, \"\")\n",
        "target_languages = [LANGUAGE_CODES.get(lang, \"\") for lang in target_languages.split(\",\")]\n",
        "if \"en\" not in target_languages:\n",
        "  target_languages.append(\"en\") # we always include english in target languages\n",
        "num_tweets = int(num_tweets)\n",
        "max_days = int(days)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "assert (query == \"\") or (users == \"\") # Can either search by query or by user, not both\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run \"config\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run \"common\"\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\n",
        "import sys\n",
        "import copy\n",
        "import tweepy\n",
        "import urllib.parse\n",
        "import sys, time, json, requests, uuid\n",
        "from tweepy import API\n",
        "from tweepy import Cursor\n",
        "from tweepy import OAuthHandler\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime, date, timedelta # don't import time here. It messes with the default library\n",
        "from azure.cosmos import CosmosClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "client = CosmosClient(COSMOS_URL, {'masterKey': COSMOS_KEY})\n",
        "database = client.get_database_client(COSMOS_DATABASE_NAME)\n",
        "tweet_container_client = database.get_container_client(container=COSMOS_CONTAINER_NAME)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET_KEY)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
        "auth_api = API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "regions = [\"Africa\",\"Arabian Gulf\",\"Asia\",\"Central America\",\"Europe\",\"Middle East\",\"North America\",\"Oceania\",\"South America\"]\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def remove_emojis(data):\n",
        "    emoji = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "        \"]+\", re.UNICODE)\n",
        "    return re.sub(emoji, '', data)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def build_entry(topic, status, query_search, container, query_language=None, target_languages=[], username=\"\"):\n",
        "    tweet = status._json\n",
        "    id_str = str(int(tweet[\"id_str\"])+abs(hash(topic)))\n",
        "    # Check for tweets which have already been added\n",
        "    search_query = 'select * from items where items.id=\"{0}\"'.format(id_str)\n",
        "    items = list(container.query_items(search_query,enable_cross_partition_query=True))\n",
        "    dt2 = datetime.now()\n",
        "    ts = int(time.mktime(dt2.timetuple()))\n",
        "    at = dt2.strftime(\"%m/%d/%Y, %H:%M:%S %Z\")\n",
        "    tweet['inserted_to_CosmosDB_at'] = at\n",
        "    tweet['inserted_to_CosmosDB_ts'] = ts\n",
        "    new_tweet = True\n",
        "    if len(items) > 0:\n",
        "        # For existing tweets, assuming tweet text the same, so don't re-process\n",
        "        # Update only the retweet and favorite counts \n",
        "        print(\"Old Tweet\")\n",
        "        updatedtweet = items[0]\n",
        "        updatedtweet[\"retweet_count\"] = tweet[\"retweet_count\"]\n",
        "        updatedtweet[\"favorite_count\"] = tweet[\"favorite_count\"]\n",
        "        tweet = updatedtweet\n",
        "        user_obj = None # don't need to re-insert the user if already seen before\n",
        "        new_tweet = False\n",
        "    elif not(tweet[\"full_text\"].lower().startswith(\"rt \")):\n",
        "        print(\"New Tweet\")\n",
        "        new_tweet = True\n",
        "        tweet[\"originalid\"] = tweet[\"id\"]\n",
        "        tweet[\"id\"] = str(int(tweet[\"id_str\"])+abs(hash(topic))) # artifically creating our own ID\n",
        "        tweet[\"topickey\"] = topic\n",
        "        tweet[\"subtopic\"] = subtopic\n",
        "        tweet[\"month_year\"] = str(str(parse(tweet[\"created_at\"]).month) + \"_\"+str(parse(tweet[\"created_at\"]).year))\n",
        "        tweet[\"replies\"]=[] # check if we want to keep this like this\n",
        "        tmp_text = tweet[\"full_text\"].replace('\\n','. ').replace('\\r','.').replace('..','. ').replace(',.','. ').replace(';.','. ').replace('?.','. ').replace('!.','. ').replace(':.','. ').lstrip('.').lstrip(' ')\n",
        "        tmp_text = remove_emojis(tmp_text)\n",
        "        tweet[\"text\"]= tmp_text\n",
        "        tweet[\"document_type\"] = \"tweet\"\n",
        "        if query_search:\n",
        "            tweet[\"search_type\"]='Topic Search'  \n",
        "            tweet[\"query\"] = query\n",
        "        else:\n",
        "            tweet[\"search_type\"]='User Search'\n",
        "            tweet[\"searched_username\"]=username\n",
        "        tweet_text = tweet[\"text\"]\n",
        "        # get translation\n",
        "        if not(query_language):\n",
        "            # will depend on language detection from the translator call\n",
        "            translations, query_language = get_translation(tweet_text, target_languages)\n",
        "        else:\n",
        "            translations, _ = get_translation(tweet_text, target_languages)\n",
        "        tweet[\"translations\"] = translations\n",
        "        # get named entities. Arabic only supports Person, Location and Organization entities, and seems to be poor, so doing NER on English text\n",
        "        named_entity_obj = {}\n",
        "        if query_language != \"en\":\n",
        "            named_entities = get_ner(translations[\"en\"])\n",
        "            org_language_entities = []\n",
        "            for ent in named_entities:\n",
        "                org_language_ent = copy.deepcopy(ent)\n",
        "                org_language_ent[\"text\"] = get_translation(ent[\"text\"], query_language)[0][query_language] # replace with original language text\n",
        "                org_language_entities.append(org_language_ent)\n",
        "            named_entity_obj[query_language] = org_language_entities\n",
        "        else:\n",
        "            named_entities = get_ner(tweet_text) # list of objects where each object corresponds to an entity\n",
        "        # add location information from azure maps\n",
        "        named_entities_with_location = []\n",
        "        for ent in named_entities:\n",
        "            new_ent = copy.deepcopy(ent)\n",
        "            if ent[\"category\"] == \"Location\" and ent[\"subcategory\"] == \"GPE\":\n",
        "                ent_text = ent[\"text\"]\n",
        "                if ent_text not in regions:\n",
        "                    # pass to Azure Maps to get country\n",
        "                    r_json = get_maps_response(ent_text)\n",
        "                    if r_json: # i.e. got a response\n",
        "                        if r_json[\"summary\"][\"numResults\"] > 0:\n",
        "                            if \"address\" in r_json['results'][0].keys():\n",
        "                                top_match = r_json['results'][0][\"address\"]\n",
        "                                if \"country\" in top_match.keys() and \"countryCode\" in top_match.keys() :\n",
        "                                    # there is a location detected, so get the country\n",
        "                                    country = top_match[\"country\"]\n",
        "                                    country_code = top_match[\"countryCode\"]\n",
        "                                    new_ent[\"country_azuremaps\"] = country\n",
        "                                    new_ent[\"country_code_azuremaps\"] = country_code\n",
        "            named_entities_with_location.append(new_ent)\n",
        "        named_entity_obj[\"en\"] = named_entities_with_location\n",
        "        for language in target_languages:\n",
        "            if language != \"en\":\n",
        "                tmp_entities = []\n",
        "                for ent in named_entities:\n",
        "                    tmp_ = copy.deepcopy(ent)\n",
        "                    tmp_[\"text\"] = get_translation(ent[\"text\"],language)[0][language]\n",
        "                    tmp_entities.append(tmp_)\n",
        "                named_entity_obj[language]=tmp_entities\n",
        "        tweet[\"named_entities\"] = named_entity_obj\n",
        "        # get sentiment. Not supported for arabic, so do on english. No need to translate back\n",
        "        if query_language != \"en\":\n",
        "            sentiment, sentiment_score = get_sentiment(translations[\"en\"])\n",
        "        else:\n",
        "            sentiment, sentiment_score = get_sentiment(tweet_text)\n",
        "        tweet[\"sentiment\"] = {\"sentiment\": sentiment, \"score\": sentiment_score}\n",
        "        user_obj = tweet[\"user\"]\n",
        "        user_obj[\"topickey\"] = topic\n",
        "        user_obj[\"id\"] = user_obj[\"id_str\"]\n",
        "        user_obj[\"document_type\"] = \"user\"\n",
        "        user_obj['inserted_to_CosmosDB_at'] = at\n",
        "        user_obj['inserted_to_CosmosDB_ts'] = ts\n",
        "        user_obj[\"month_year\"] = str(str(parse(user_obj[\"created_at\"]).month) + \"_\"+str(parse(user_obj[\"created_at\"]).year))\n",
        "        user_location = tweet[\"user\"][\"location\"]\n",
        "        if user_location != \"\" and user_location not in regions:\n",
        "            r_json = get_maps_response(user_location)\n",
        "            if r_json: # i.e. got a response\n",
        "                if r_json[\"summary\"][\"numResults\"] > 0:\n",
        "                    # there is a location detected, so get the country\n",
        "                    if \"address\" in r_json['results'][0].keys():\n",
        "                        top_match = r_json['results'][0][\"address\"]\n",
        "                        if \"country\" in top_match.keys() and \"countryCode\" in top_match.keys():\n",
        "                            country = top_match[\"country\"]\n",
        "                            country_code = top_match[\"countryCode\"]\n",
        "                            user_obj[\"country_azuremaps\"] = country\n",
        "                            user_obj[\"country_code_azuremaps\"] = country_code\n",
        "        tweet[\"userid\"]=user_obj[\"id\"]\n",
        "    else:\n",
        "        return None, None, False\n",
        "    return tweet, user_obj, new_tweet\n",
        "def process_tweets(topic=\"\", query=\"\", language=\"en\", maxdays=365, maxtweets_persearch=1, user=\"\", query_search=True, container=None, target_languages=[], username=\"\"):\n",
        "    print(\"Working on topic: \" + topic)\n",
        "    end_date = datetime.utcnow() - timedelta(days=maxdays)\n",
        "    all_tweets = []\n",
        "    all_users = []\n",
        "    count = 0\n",
        "    # Reference: https://docs.tweepy.org/en/stable/api.html#API.search\n",
        "    if query_search:\n",
        "    # searching based on the query string\n",
        "        if language:\n",
        "            for status in Cursor(auth_api.search, q=query, lang=language, result='recent', tweet_mode = \"extended\", include_rts='False').items(maxtweets_persearch):\n",
        "                count += 1\n",
        "                tweet_obj, user_obj, new_tweet = build_entry(topic, status, query_search, container, language, target_languages)\n",
        "                print(new_tweet)\n",
        "                all_tweets.append(tweet_obj)\n",
        "                if new_tweet:\n",
        "                    all_users.append(user_obj)\n",
        "                if status.created_at < end_date:\n",
        "                    break\n",
        "            print(\"Found \"+str(count) +\" tweets for query: \"+ query)\n",
        "            return all_tweets, all_users\n",
        "        else: # search for tweets regardless of tweet language\n",
        "            for status in Cursor(auth_api.search, q=query, result='recent', tweet_mode='extended').items(maxtweets_persearch):\n",
        "                count += 1\n",
        "                tweet_obj, user_obj, new_tweet = build_entry(topic, status, query_search, container, query_language=None, target_languages=target_languages)\n",
        "                all_tweets.append(tweet_obj)\n",
        "                all_users.append(user_obj)\n",
        "                if status.created_at < end_date:\n",
        "                    break \n",
        "            print(\"Found \"+str(count) +\" tweets for query: \"+ query)\n",
        "            return all_tweets, all_users\n",
        "    else:\n",
        "    # getting tweets by user\n",
        "        for status in Cursor(auth_api.user_timeline, id=user, result='recent', tweet_mode = \"extended\").items(maxtweets_persearch): # this actually only returns the last 20 tweets by the user\n",
        "            count += 1\n",
        "            tweet_obj, user_obj, new_tweet = build_entry(topic, status, query_search, container, query_language=None, target_languages=target_languages, username=username)\n",
        "            all_tweets.append(tweet_obj)\n",
        "            print(new_tweet)\n",
        "            if new_tweet:\n",
        "                all_users.append(user_obj) # todo: optimize so not re-inserting the same user\n",
        "            if status.created_at < end_date:\n",
        "                break\n",
        "        print(\"Found \"+str(count) +\" tweets for user: \"+ user)\n",
        "        return all_tweets, all_users\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if users == \"\": # query search\n",
        "    all_tweets, all_users = process_tweets(topic, query, query_language, maxdays=max_days, maxtweets_persearch=num_tweets, user=\"\", query_search=True, container=tweet_container_client, target_languages=target_languages)\n",
        "    update_cosmos(all_tweets, tweet_container_client) # insert tweets\n",
        "    update_cosmos(all_users, tweet_container_client) # insert users\n",
        "else: # user search\n",
        "    for usr in userslist:\n",
        "        all_tweets, all_users = process_tweets(topic, user=usr, maxdays=max_days, maxtweets_persearch=num_tweets, query_search=False, container=tweet_container_client, target_languages=target_languages, username=usr)\n",
        "        update_cosmos(all_tweets, tweet_container_client) # insert tweets\n",
        "        update_cosmos(all_users, tweet_container_client) # insert users\n",
        ""
      ],
      "attachments": {}
    }
  ]
}